{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn+XOUyYHk4oe9WwMq+3fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishika-244/ishika-244/blob/main/Data_Automation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Libraries**"
      ],
      "metadata": {
        "id": "c2Q-3IYezhRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uXwAT1a1ULT",
        "outputId": "80760119-2020-450b-f29e-369ded3f8451"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrX43tf_zAjR",
        "outputId": "fa1e7832-cdee-4321-afc6-1add64b17581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.54)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install yfinance pandas numpy schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**importing libraries**"
      ],
      "metadata": {
        "id": "DmKXLrBRz6Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import schedule\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "9-unhsBHz-_h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Data**"
      ],
      "metadata": {
        "id": "4V5jvsK70NCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('cleaned_stock_data.csv')"
      ],
      "metadata": {
        "id": "G9kD5y_T0Qmg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetching and Cleaning Live TCS Data**"
      ],
      "metadata": {
        "id": "FHJpxZHy0eds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_clean_tcs():\n",
        "  try:\n",
        "    # Define Stock Symbol\n",
        "    stock_symbol = \"TCS.NS\"\n",
        "    stock_data = yf.Ticker(stock_symbol)\n",
        "\n",
        "    # Fetch latest stock data for today\n",
        "    df = stock_data.history(period=\"1d\")\n",
        "\n",
        "    # check if data was successfully fetched\n",
        "    if stock_data.empty:\n",
        "      print(\"No data fetched for TCS.NS\")\n",
        "      return\n",
        "\n",
        "    # Format the data\n",
        "    stock_data.insert(0 , \"Date\" , df.index.date)\n",
        "    stock_data = stock_data[[('Date', 'Open', 'High', 'Low', 'Close', 'Volume','Dividends','sma_50','sma_200','ema_50','RSI','ATR','MACD','nifty_close','crude_oil','usd_inr','daily_range','Close_Smoothed')]]\n",
        "\n",
        "\n",
        "    # Data Cleaning abd Validation Steps\n",
        "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "\n",
        "    invalid_prices = stock_data[(stock_data['High']<stock_data['Low']) |\n",
        "                         (stock_data['Open']>stock_data['High']) | (stock_data['Open'] < stock_data['Low']) |\n",
        "                         (stock_data['Close']>stock_data['High']) | (stock_data['Close'] < stock_data['Low'])]\n",
        "     # Validate Price Relationship\n",
        "    if not invalid_prices.empty:\n",
        "           print(\"Invalid Price Relationship Found:\")\n",
        "           print(invalid_prices)\n",
        "    else:\n",
        "        print(\"No Invalid Price Relationship Found.\")\n",
        "\n",
        "\n",
        "    # Check for zero volumes\n",
        "    invalid_volumes = stock_data[stock_data['Volume'] <=0]\n",
        "    if not invalid_volumes.empty:\n",
        "     print(\"Invalid Volumes Found , replacing 0 with mean volume...:\")\n",
        "     stock_data['Volume'] = stock_data['Volume'].replace(0,stock_data['Volume'].mean())\n",
        "     print(\"Invalid Volumes Handled\")\n",
        "    else:\n",
        "      print(\"No Invalid Volumes Found.\")\n",
        "\n",
        "\n",
        "    # Check for Negative Volumes\n",
        "    stock_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'sma_50', 'sma_200',\n",
        "                 'ema_50', 'RSI', 'MACD', 'ATR', 'nifty_close', 'usd_inr', 'crude_oil']\n",
        "    negative_values = stock_data[stock_columns] < 0\n",
        "    if negative_values.any().any():\n",
        "     print(\"Negative Values Found , replaced with Absolute Values\")\n",
        "     stock_data[stock_columns] = stock_data[stock_columns].abs()\n",
        "     print(\"Negative Values Handled\")\n",
        "    else:\n",
        "     print(\"No Negative Values Found.\")\n",
        "\n",
        "\n",
        "    # Validate SMA Calculation\n",
        "    stock_data['sma_50_check'] =stock_data['Close'].rolling(window=50,adjust=False).mean()\n",
        "    sma_mismatch = stock_data[abs(stock_data['sma_50'] - stock_data['sma_50_check']) > 1e-2]  # Allowing small rounding error\n",
        "    if not sma_mismatch.empty:\n",
        "     print(\"SMA Mismatch Found:\")\n",
        "     stock_data['sma_50'] = stock_data['Close'].rolling(window=50, adjust=False).mean()\n",
        "     print(sma_mismatch)\n",
        "    else:\n",
        "     print(\"No SMA Mismatch Found.\")\n",
        "\n",
        "\n",
        "    # Validate EMA Calculation\n",
        "    stock_data['ema_50_check'] = stock_data['Close'].ewm(span=50, adjust=False).mean()\n",
        "    ema_mismatch = stock_data[abs(stock_data['ema_50'] - stock_data['ema_50_check']) > 1e-2]\n",
        "    # Print results\n",
        "    if not ema_mismatch.empty:\n",
        "     print(\"⚠️ EMA Mismatch Found and handled\")\n",
        "     stock_data['ema_50'] = stock_data['Close'].ewm(span=50, adjust=False).mean()\n",
        "    # print(ema_mismatch[['Date', 'ema_50', 'ema_50_check']])\n",
        "    else:\n",
        "     print(\"✅ No EMA Mismatch Found.\")\n",
        "\n",
        "\n",
        "    # Calculate RSI (Relative Strength Index)\n",
        "    delta = stock_data['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    stock_data['RSI_check'] = 100 - (100 / (1 + rs))\n",
        "    # Check for mismatches\n",
        "    rsi_mismatch = stock_data[abs(stock_data['RSI'] - stock_data['RSI_check']) > 1e-2]\n",
        "    # Print results\n",
        "    if not rsi_mismatch.empty:\n",
        "     print(\"⚠️ RSI Mismatch Found and handled\")\n",
        "     stock_data['RSI'] = 100 - (100 / (1 + (stock_data['Close'].diff().where(stock_data['Close'].diff() > 0, 0).rolling(window=14).mean() / (-stock_data['Close'].diff().where(stock_data['Close'].diff() < 0, 0).rolling(window=14).mean()))))\n",
        "    # print(rsi_mismatch[['Date', 'RSI', 'RSI_check']])\n",
        "    else:\n",
        "     print(\"✅ No RSI Mismatch Found.\")\n",
        "\n",
        "\n",
        "    # Calculate MACD (Moving Average Convergence Divergence)\n",
        "    stock_data['macd_line_check'] = stock_data['Close'].ewm(span=12, adjust=False).mean() - stock_data['Close'].ewm(span=26, adjust=False).mean()\n",
        "    # Check for mismatches\n",
        "    macd_mismatch = stock_data[abs(stock_data['MACD'] - stock_data['macd_line_check']) >1e-2]\n",
        "    # Print results\n",
        "    if not macd_mismatch.empty:\n",
        "     print(\"⚠️ MACD Mismatch Found and handled\")\n",
        "     stock_data['MACD'] = stock_data['Close'].ewm(span=12, adjust=False).mean() - stock_data['Close'].ewm(span=26, adjust=False).mean()\n",
        "    # print(macd_mismatch[['Date', 'MACD', 'macd_line_check']])\n",
        "    else:\n",
        "     print(\"✅ No MACD Mismatch Found.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate True Range (TR)\n",
        "    stock_data['TR'] = np.maximum(\n",
        "    stock_data['High'] - stock_data['Low'],\n",
        "    np.maximum(abs(stock_data['High'] - stock_data['Close'].shift(1)),\n",
        "               abs(stock_data['Low'] - stock_data['Close'].shift(1)))\n",
        "                )\n",
        "   # Calculate ATR (Average True Range)\n",
        "    stock_data['ATR_check'] = stock_data['TR'].rolling(window=14).mean()\n",
        "   # Check for mismatches\n",
        "    atr_mismatch = stock_data[abs(stock_data['ATR'] - stock_data['ATR_check']) > 1e-2]\n",
        "    # Print results\n",
        "    if not atr_mismatch.empty:\n",
        "     print(\"⚠️ ATR Mismatch Found and handled:\")\n",
        "     stock_data['ATR'] = pd.concat([(stock_data['High'] - stock_data['Low']), abs(stock_data['High'] - stock_data['Close'].shift(1)), abs(stock_data['Low'] - stock_data['Close'].shift(1))], axis=1).max(axis=1).rolling(window=14).mean()\n",
        "     # print(atr_mismatch[['Date', 'ATR', 'ATR_check']])\n",
        "    else:\n",
        "     print(\"✅ No ATR Mismatch Found.\")\n",
        "\n",
        "\n",
        "     # Price Spikes\n",
        "    stock_data['daily_range'] = stock_data['High'] - stock_data['Low']\n",
        "    threshold = stock_data['daily_range'].mean() + 3 * stock_data['daily_range'].std()  # Setting a 3-sigma limit\n",
        "    price_spikes = stock_data[stock_data['daily_range'] > threshold]\n",
        "    if not price_spikes.empty:\n",
        "     print(\"Unexpected Price Spikes Found:\")\n",
        "     print(price_spikes)\n",
        "    else:\n",
        "     print(\"No Unexpected Price Spikes Found.\")\n",
        "\n",
        "\n",
        "     # Close Column Smoothing\n",
        "    stock_data['Close_Smoothed'] = stock_data['Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    # Droppinh unecessary columns\n",
        "    stock_data.drop(columns = ['sma_50_check', 'ema_50_check', 'RSI_check', 'macd_line_check', 'ATR_check','TR' ],inplace = True)\n",
        "\n",
        "    # Changing the type of Volume from float to int\n",
        "    stock_data['Volume'] = stock_data['Volume'].astype(int)\n",
        "\n",
        "\n",
        "    # Handling Missing Values\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    def handle_missing(stock_data):\n",
        "     if stock_data.isnull().sum().sum() > 0:\n",
        "      impute = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
        "      impute.fit(stock_data[['Open','High','Low','Close','Volume','Dividends','sma_50','sma_200','ema_50','RSI','MACD','ATR','nifty_close','usd_inr','crude_oil']])\n",
        "      stock_data[['Open','High','Low','Close','Volume','Dividends','sma_50','sma_200','ema_50','RSI','MACD','ATR','nifty_close','usd_inr','crude_oil']] = impute.transform(stock_data[['Open','High','Low','Close','Volume','Dividends','sma_50','sma_200','ema_50','RSI','MACD','ATR','nifty_close','usd_inr','crude_oil']])\n",
        "     else:\n",
        "      print(\"No missing values found\")\n",
        "\n",
        "    handle_missing(stock_data)\n",
        "\n",
        "    stock_data = pd.DataFrame(stock_data)\n",
        "\n",
        "\n",
        "     #removing duplicates\n",
        "    def removing_duplicates(stock_data):\n",
        "     if stock_data.duplicated().sum()>0:\n",
        "      stock_data.drop_duplicates(inplace=True)\n",
        "     else:\n",
        "      print(\"No duplicates found\")\n",
        "\n",
        "    removing_duplicates(stock_data)\n",
        "\n",
        "    duplicate_dates = stock_data.index.duplicated().sum()\n",
        "    if duplicate_dates > 0:\n",
        "     print(f\"Duplicate Dates Found: {duplicate_dates}\")\n",
        "    else:\n",
        "     print(\"No Duplicate Dates Found.\")\n",
        "\n",
        "\n",
        "    #  Data oveview\n",
        "    stock_data.tail()\n",
        "    stock_data.info()\n",
        "    stock_data.describe()\n",
        "\n",
        "\n",
        "    #  Store Cleaned Data in CSV File\n",
        "    if not df.empty:\n",
        "      if os.path.exists(stock_data):\n",
        "       data.to_csv(stock_data, mode='a', header=False, index=False)  # Append new data\n",
        "      else:\n",
        "         data.to_csv(stock_data, index=False)  # Create a new file if not exists\n",
        "\n",
        "    print(\"Daily cleaned stock data saved successfully.\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "RZ35_icn0khM"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}